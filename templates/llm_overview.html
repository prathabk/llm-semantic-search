<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding LLMs - Semantic Search Course</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/carousel.css') }}">
    <style>
        canvas {
            border: 1px solid var(--border);
            border-radius: 8px;
            background: var(--surface);
            display: block;
            margin: 20px auto;
            box-shadow: 0 1px 3px var(--shadow);
        }

        .title-slide {
            text-align: center;
            padding: 80px 20px;
        }

        .title-slide h1 {
            font-size: 3em;
            margin-bottom: 16px;
        }

        .title-slide .subtitle {
            font-size: 1.5em;
            color: var(--text-secondary);
            margin-bottom: 40px;
        }

        .title-slide .course-info {
            max-width: 700px;
            margin: 0 auto;
            font-size: 1.125em;
            line-height: 1.8;
        }

        .word-button {
            padding: 12px 20px;
            margin: 8px;
            border: 2px solid var(--primary);
            background: var(--surface);
            color: var(--primary);
            border-radius: 8px;
            font-size: 1em;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: 500;
        }

        .word-button:hover {
            background: var(--primary);
            color: white;
            transform: translateY(-2px);
        }

        .word-button.selected {
            background: var(--primary);
            color: white;
        }

        .probability-bar {
            height: 40px;
            background: linear-gradient(90deg, var(--primary), var(--primary-dark));
            border-radius: 6px;
            margin: 8px 0;
            display: flex;
            align-items: center;
            padding: 0 12px;
            color: white;
            font-weight: 600;
            transition: width 0.5s ease;
        }

        .token-box {
            display: inline-block;
            padding: 8px 16px;
            margin: 4px;
            background: var(--primary);
            color: white;
            border-radius: 6px;
            font-family: monospace;
            font-size: 0.95em;
        }

        .attention-matrix {
            display: grid;
            gap: 4px;
            margin: 20px auto;
            max-width: 500px;
        }

        .attention-cell {
            aspect-ratio: 1;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.75em;
            color: white;
            font-weight: 600;
            transition: all 0.3s;
            cursor: pointer;
        }

        .attention-cell:hover {
            transform: scale(1.1);
            z-index: 10;
        }

        .layer-viz {
            display: flex;
            flex-direction: column;
            gap: 20px;
            max-width: 600px;
            margin: 20px auto;
        }

        .layer-box {
            padding: 20px;
            background: var(--surface);
            border: 2px solid var(--border);
            border-radius: 8px;
            text-align: center;
            transition: all 0.3s;
        }

        .layer-box.active {
            border-color: var(--primary);
            background: #eff6ff;
            transform: scale(1.02);
        }

        .stat-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }

        .stat-card {
            padding: 24px;
            background: var(--surface);
            border: 2px solid var(--border);
            border-radius: 12px;
            text-align: center;
        }

        .stat-number {
            font-size: 2.5em;
            font-weight: 700;
            color: var(--primary);
            margin: 8px 0;
        }

        .stat-label {
            color: var(--text-secondary);
            font-size: 0.95em;
        }

        .flow-arrow {
            text-align: center;
            font-size: 2em;
            color: var(--primary);
            margin: 10px 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="/" class="home-link">‚Üê Back to Home</a>

        <div class="progress-bar">
            <div class="progress-fill" id="progressFill"></div>
        </div>

        <div class="carousel-container">
            <div class="carousel-wrapper" id="carouselWrapper">

                <!-- Slide 1: Title -->
                <div class="slide">
                    <div class="slide-content title-slide">
                        <h1>ü§ñ Understanding Large Language Models</h1>
                        <p class="subtitle">How AI Predicts and Generates Text</p>
                        <div class="course-info">
                            <p>Large Language Models (LLMs) like ChatGPT, Claude, and GPT-4 have transformed how we interact with AI. But how do they actually work?</p>
                            <p>In this journey, we'll explore the fundamental concepts behind LLMs - from word prediction to transformer architecture - in an intuitive, visual way.</p>
                            <p style="margin-top: 32px; color: var(--text-secondary);">
                                Use arrow keys or buttons below to navigate
                            </p>
                        </div>
                    </div>
                </div>

                <!-- Slide 2: What is an LLM? -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>What is a Large Language Model?</h2>
                            <p class="slide-subtitle">The Core Concept</p>
                        </div>

                        <p style="font-size: 1.125em; line-height: 1.8;">
                            At its heart, an LLM is a <span class="highlight">sophisticated mathematical function that predicts what word comes next</span> for any piece of text.
                        </p>

                        <div class="example-box" style="margin: 30px 0;">
                            <h3>Think of it like this:</h3>
                            <p style="font-size: 1.05em;">
                                When you type "The cat sat on the..." your brain automatically suggests words like "mat," "floor," or "couch."
                                An LLM does exactly this, but instead of picking one word with certainty, it assigns <span class="highlight">probabilities</span> to thousands of possible next words.
                            </p>
                        </div>

                        <div style="background: #eff6ff; padding: 24px; border-radius: 8px; border-left: 4px solid var(--primary);">
                            <p style="font-size: 1.05em; margin: 0;">
                                <strong>Key Insight:</strong> LLMs don't "understand" text like humans do. They're incredibly good at pattern matching across billions of examples they've seen during training.
                            </p>
                        </div>

                        <p style="margin-top: 30px; color: var(--text-secondary);">
                            Let's see this in action on the next slide...
                        </p>
                    </div>
                </div>

                <!-- Slide 3: Interactive Word Prediction -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>The Prediction Game</h2>
                            <p class="slide-subtitle">How LLMs Choose Words</p>
                        </div>

                        <p style="text-align: center; margin-bottom: 20px;">
                            <strong>Input text:</strong> "The weather today is"
                        </p>

                        <div style="background: var(--surface); padding: 30px; border-radius: 12px; border: 2px solid var(--border);">
                            <p style="text-align: center; margin-bottom: 20px; font-weight: 600;">
                                Click a word to see how the probabilities update:
                            </p>

                            <div style="text-align: center; margin: 20px 0;">
                                <button class="word-button" onclick="selectWord('sunny')">sunny</button>
                                <button class="word-button" onclick="selectWord('rainy')">rainy</button>
                                <button class="word-button" onclick="selectWord('cold')">cold</button>
                            </div>

                            <div id="probabilityViz" style="margin-top: 30px;">
                                <!-- Probability bars will be inserted here -->
                            </div>
                        </div>

                        <div class="example-box" style="margin-top: 30px;">
                            <p><strong>What's happening:</strong> The LLM calculates a probability for every word in its vocabulary (often 50,000+ words). It then randomly samples from this distribution, which is why you can get different responses each time.</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 4: From Words to Numbers -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>From Words to Numbers</h2>
                            <p class="slide-subtitle">Tokenization and Embeddings</p>
                        </div>

                        <p style="font-size: 1.05em; line-height: 1.8;">
                            Computers can't work directly with words - they need numbers. This happens in two steps:
                        </p>

                        <div class="layer-viz">
                            <div class="layer-box">
                                <h3 style="margin-top: 0;">Step 1: Tokenization</h3>
                                <p>Break text into chunks called "tokens"</p>
                                <div style="margin: 16px 0;">
                                    <span class="token-box">The</span>
                                    <span class="token-box">cat</span>
                                    <span class="token-box">sat</span>
                                </div>
                                <p style="font-size: 0.9em; color: var(--text-secondary);">Each token gets a unique ID number</p>
                            </div>

                            <div class="flow-arrow">‚Üì</div>

                            <div class="layer-box">
                                <h3 style="margin-top: 0;">Step 2: Embedding</h3>
                                <p>Convert each token ID into a high-dimensional vector</p>
                                <canvas id="embeddingCanvas" width="500" height="200"></canvas>
                                <p style="font-size: 0.9em; color: var(--text-secondary); margin-top: 12px;">
                                    Each word becomes a point in space (typically 768-12,288 dimensions!)
                                </p>
                            </div>
                        </div>

                        <div style="background: #fef3c7; padding: 20px; border-radius: 8px; margin-top: 24px;">
                            <p style="margin: 0;"><strong>üí° Why vectors?</strong> Similar words end up close together in this space. "cat" and "dog" are nearby, while "cat" and "mathematics" are far apart.</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 5: The Transformer Architecture -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>The Transformer Architecture</h2>
                            <p class="slide-subtitle">The Brain of Modern LLMs</p>
                        </div>

                        <p style="font-size: 1.05em;">
                            Once words are converted to vectors, they flow through a <span class="highlight">transformer</span> - a special neural network architecture with two key components:
                        </p>

                        <div class="layer-viz">
                            <div class="layer-box" id="layer1">
                                <h3 style="margin-top: 0; color: var(--primary);">üîç Attention Mechanism</h3>
                                <p style="margin: 12px 0;">Allows each word to "look at" and gather information from other words in the context.</p>
                                <p style="font-size: 0.95em; color: var(--text-secondary);">
                                    Example: In "The animal didn't cross the street because it was too tired",
                                    attention helps link "it" ‚Üí "animal"
                                </p>
                            </div>

                            <div class="flow-arrow">‚Üì</div>

                            <div class="layer-box" id="layer2">
                                <h3 style="margin-top: 0; color: var(--primary);">üß† Feed-Forward Network</h3>
                                <p style="margin: 12px 0;">Processes each word independently, applying learned patterns about language.</p>
                                <p style="font-size: 0.95em; color: var(--text-secondary);">
                                    This is where most of the model's knowledge is stored - billions of parameters encode language patterns
                                </p>
                            </div>
                        </div>

                        <div class="example-box" style="margin-top: 30px;">
                            <p><strong>Key Innovation:</strong> Unlike older models that read text sequentially (word by word), transformers process <span class="highlight">all words simultaneously</span> using attention. This parallel processing is much faster and captures context better.</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 6: Attention Visualization -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Understanding Attention</h2>
                            <p class="slide-subtitle">Interactive Visualization</p>
                        </div>

                        <p style="text-align: center; margin-bottom: 20px;">
                            <strong>Sentence:</strong> "The cat sat on the mat"
                        </p>

                        <div style="background: var(--surface); padding: 30px; border-radius: 12px; border: 2px solid var(--border);">
                            <p style="text-align: center; margin-bottom: 20px; font-weight: 600;">
                                Hover over cells to see attention strengths
                            </p>

                            <canvas id="attentionCanvas" width="600" height="400"></canvas>

                            <div style="margin-top: 20px; text-align: center; color: var(--text-secondary);">
                                <p style="margin: 8px 0;">Darker connections = stronger attention</p>
                                <p style="font-size: 0.9em;">Click on a word to highlight its attention pattern</p>
                            </div>
                        </div>

                        <div style="background: #eff6ff; padding: 20px; border-radius: 8px; margin-top: 24px; border-left: 4px solid var(--primary);">
                            <p style="margin: 0;"><strong>What you're seeing:</strong> Attention weights show how much each word "pays attention to" every other word. The model has learned these patterns from billions of examples.</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 7: Training Process -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>How LLMs Learn</h2>
                            <p class="slide-subtitle">The Training Process</p>
                        </div>

                        <p style="font-size: 1.05em; line-height: 1.8;">
                            Training an LLM is like teaching it to fill in the blanks across billions of examples:
                        </p>

                        <canvas id="trainingCanvas" width="600" height="300"></canvas>

                        <div class="layer-viz" style="margin-top: 30px;">
                            <div class="layer-box">
                                <h3 style="margin-top: 0;">1Ô∏è‚É£ Start with Random Weights</h3>
                                <p>The model begins knowing nothing - its predictions are gibberish</p>
                            </div>

                            <div class="flow-arrow">‚Üì</div>

                            <div class="layer-box">
                                <h3 style="margin-top: 0;">2Ô∏è‚É£ Make Predictions</h3>
                                <p>Given "The cat sat on the ___", predict the next word</p>
                            </div>

                            <div class="flow-arrow">‚Üì</div>

                            <div class="layer-box">
                                <h3 style="margin-top: 0;">3Ô∏è‚É£ Calculate Error</h3>
                                <p>Compare prediction to actual next word, measure how wrong it was</p>
                            </div>

                            <div class="flow-arrow">‚Üì</div>

                            <div class="layer-box">
                                <h3 style="margin-top: 0;">4Ô∏è‚É£ Update Weights (Backpropagation)</h3>
                                <p>Slightly adjust billions of parameters to improve future predictions</p>
                            </div>
                        </div>

                        <div class="example-box" style="margin-top: 30px;">
                            <p><strong>Repeat billions of times:</strong> This process runs across massive datasets for weeks or months, gradually refining the model's ability to predict text.</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 8: Scale and Parameters -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>The Scale of Modern LLMs</h2>
                            <p class="slide-subtitle">Mind-Boggling Numbers</p>
                        </div>

                        <p style="text-align: center; font-size: 1.1em; margin-bottom: 30px;">
                            Modern LLMs operate at a scale that's difficult to comprehend:
                        </p>

                        <div class="stat-grid">
                            <div class="stat-card">
                                <div class="stat-number">175B+</div>
                                <div class="stat-label">Parameters (GPT-3)</div>
                                <p style="margin-top: 12px; font-size: 0.9em; color: var(--text-secondary);">
                                    Each parameter is a number that gets adjusted during training
                                </p>
                            </div>

                            <div class="stat-card">
                                <div class="stat-number">300B+</div>
                                <div class="stat-label">Training Tokens</div>
                                <p style="margin-top: 12px; font-size: 0.9em; color: var(--text-secondary);">
                                    Would take a human 2,600+ years to read
                                </p>
                            </div>

                            <div class="stat-card">
                                <div class="stat-number">96</div>
                                <div class="stat-label">Transformer Layers</div>
                                <p style="margin-top: 12px; font-size: 0.9em; color: var(--text-secondary);">
                                    Data flows through dozens of attention + feed-forward layers
                                </p>
                            </div>

                            <div class="stat-card">
                                <div class="stat-number">100M+</div>
                                <div class="stat-label">Years of Computation</div>
                                <p style="margin-top: 12px; font-size: 0.9em; color: var(--text-secondary);">
                                    If done sequentially on a single computer
                                </p>
                            </div>
                        </div>

                        <div style="background: #fef3c7; padding: 24px; border-radius: 8px; margin-top: 30px;">
                            <p style="margin: 0; font-size: 1.05em;">
                                <strong>üí° Emergent Abilities:</strong> At this scale, LLMs develop capabilities that weren't explicitly programmed - like reasoning, translation, and code generation. These emerge from pattern matching across vast amounts of data.
                            </p>
                        </div>
                    </div>
                </div>

                <!-- Slide 9: Two-Stage Training -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Two-Stage Training Process</h2>
                            <p class="slide-subtitle">From Text Prediction to Conversation</p>
                        </div>

                        <div class="layer-viz">
                            <div class="layer-box" style="background: #eff6ff;">
                                <h3 style="margin-top: 0; color: var(--primary);">Stage 1: Pre-training</h3>
                                <div style="text-align: left; max-width: 500px; margin: 16px auto;">
                                    <p><strong>Task:</strong> Predict the next word</p>
                                    <p><strong>Data:</strong> Massive internet text (books, websites, code)</p>
                                    <p><strong>Duration:</strong> Weeks to months on thousands of GPUs</p>
                                    <p><strong>Result:</strong> Model learns general language patterns</p>
                                </div>
                            </div>

                            <div class="flow-arrow">‚Üì</div>

                            <div class="layer-box" style="background: #f0fdf4;">
                                <h3 style="margin-top: 0; color: #10b981;">Stage 2: Fine-tuning (RLHF)</h3>
                                <div style="text-align: left; max-width: 500px; margin: 16px auto;">
                                    <p><strong>Task:</strong> Learn to be helpful, harmless, and honest</p>
                                    <p><strong>Data:</strong> Human-labeled examples and feedback</p>
                                    <p><strong>Method:</strong> Reinforcement Learning with Human Feedback</p>
                                    <p><strong>Result:</strong> Model learns to follow instructions and chat naturally</p>
                                </div>
                            </div>
                        </div>

                        <div class="example-box" style="margin-top: 30px;">
                            <h3>Why two stages?</h3>
                            <p>Pre-training gives the model broad knowledge of language. Fine-tuning teaches it to use that knowledge in ways that are helpful and aligned with human values. Without fine-tuning, a pre-trained model would just complete text without following instructions.</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 10: Applications and Connections -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>LLMs in Semantic Search</h2>
                            <p class="slide-subtitle">Connecting to Our Course</p>
                        </div>

                        <p style="font-size: 1.1em; line-height: 1.8;">
                            Now that you understand how LLMs work, here's how they power semantic search:
                        </p>

                        <div class="layer-viz">
                            <div class="layer-box" style="background: #eff6ff;">
                                <h3 style="margin-top: 0;">üß¨ Embeddings from LLMs</h3>
                                <p>We use the internal representations (embeddings) from trained LLMs to convert text into meaningful vectors</p>
                            </div>

                            <div class="flow-arrow">‚Üì</div>

                            <div class="layer-box" style="background: #fef3c7;">
                                <h3 style="margin-top: 0;">üìä Semantic Understanding</h3>
                                <p>These embeddings capture semantic meaning, not just keywords. "Dog" and "puppy" have similar vectors even though the words are different</p>
                            </div>

                            <div class="flow-arrow">‚Üì</div>

                            <div class="layer-box" style="background: #f0fdf4;">
                                <h3 style="margin-top: 0;">üîç Similarity Search</h3>
                                <p>We can find documents by comparing query embeddings to document embeddings using vector similarity (cosine similarity, dot product)</p>
                            </div>
                        </div>

                        <div class="example-box" style="margin-top: 30px;">
                            <h3>Real-World Applications</h3>
                            <ul style="text-align: left; line-height: 1.8;">
                                <li>Search engines that understand intent</li>
                                <li>Document retrieval systems</li>
                                <li>Recommendation engines</li>
                                <li>Question-answering systems</li>
                                <li>Content similarity detection</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- Slide 11: Key Takeaways -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Key Takeaways</h2>
                            <p class="slide-subtitle">What You've Learned</p>
                        </div>

                        <ul style="font-size: 1.125em; line-height: 2; max-width: 700px; margin: 40px auto;">
                            <li><strong>LLMs predict text:</strong> At their core, they're sophisticated functions that predict the next word based on context</li>

                            <li><strong>Everything becomes vectors:</strong> Words are tokenized and converted to high-dimensional embeddings</li>

                            <li><strong>Transformers use attention:</strong> The attention mechanism allows models to understand context by relating words to each other</li>

                            <li><strong>Training is massive:</strong> LLMs learn from hundreds of billions of tokens across weeks of training on specialized hardware</li>

                            <li><strong>Two-stage process:</strong> Pre-training builds knowledge, fine-tuning aligns behavior</li>

                            <li><strong>Embeddings enable semantic search:</strong> The internal representations learned by LLMs power modern search systems</li>
                        </ul>

                        <div style="text-align: center; margin-top: 60px;">
                            <a href="/llm-models" style="text-decoration: none;">
                                <button class="action-btn" style="padding: 16px 32px; font-size: 1.125em;">
                                    Next: Using LLMs with Ollama ‚Üí
                                </button>
                            </a>
                        </div>
                    </div>
                </div>

            </div>
        </div>

        <!-- Navigation Controls -->
        <div class="carousel-nav">
            <button id="prevBtn" onclick="prevSlide()">‚Üê Previous</button>

            <div class="nav-center">
                <div class="slide-indicators" id="slideIndicators"></div>
                <span class="slide-counter">
                    <span id="currentSlide">1</span> / <span id="totalSlides">11</span>
                </span>
            </div>

            <button id="nextBtn" onclick="nextSlide()">Next ‚Üí</button>
        </div>
    </div>

    <div class="keyboard-hint" id="keyboardHint">
        Use <kbd>‚Üê</kbd> <kbd>‚Üí</kbd> arrow keys to navigate
    </div>

    <script>
        // ========== CAROUSEL FUNCTIONALITY ==========
        let currentSlide = 0;
        const totalSlides = 11;
        const carouselWrapper = document.getElementById('carouselWrapper');
        const progressFill = document.getElementById('progressFill');
        const prevBtn = document.getElementById('prevBtn');
        const nextBtn = document.getElementById('nextBtn');

        function initCarousel() {
            updateSlide();
            createIndicators();
            setTimeout(() => {
                document.getElementById('keyboardHint').style.animation = 'fadeInOut 3s ease-in-out';
            }, 1000);
        }

        function nextSlide() {
            if (currentSlide < totalSlides - 1) {
                currentSlide++;
                updateSlide();
            }
        }

        function prevSlide() {
            if (currentSlide > 0) {
                currentSlide--;
                updateSlide();
            }
        }

        function goToSlide(index) {
            currentSlide = index;
            updateSlide();
        }

        function updateSlide() {
            const offset = -currentSlide * 100;
            carouselWrapper.style.transform = `translateX(${offset}%)`;

            const progress = ((currentSlide + 1) / totalSlides) * 100;
            progressFill.style.width = `${progress}%`;

            document.getElementById('currentSlide').textContent = currentSlide + 1;

            prevBtn.disabled = currentSlide === 0;
            nextBtn.disabled = currentSlide === totalSlides - 1;

            updateIndicators();

            // Reinitialize visualizations when entering specific slides
            setTimeout(() => {
                if (currentSlide === 3) drawEmbeddingVisualization();
                if (currentSlide === 5) drawAttentionVisualization();
                if (currentSlide === 6) drawTrainingVisualization();
            }, 100);
        }

        function createIndicators() {
            const container = document.getElementById('slideIndicators');
            for (let i = 0; i < totalSlides; i++) {
                const indicator = document.createElement('div');
                indicator.className = 'slide-indicator';
                indicator.onclick = () => goToSlide(i);
                container.appendChild(indicator);
            }
            updateIndicators();
        }

        function updateIndicators() {
            const indicators = document.querySelectorAll('.slide-indicator');
            indicators.forEach((indicator, index) => {
                indicator.classList.toggle('active', index === currentSlide);
            });
        }

        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight') nextSlide();
            if (e.key === 'ArrowLeft') prevSlide();
        });

        // ========== WORD PREDICTION VISUALIZATION (Slide 3) ==========
        const predictions = {
            'sunny': [
                { word: 'and', prob: 35 },
                { word: 'warm', prob: 28 },
                { word: ',', prob: 15 },
                { word: 'with', prob: 12 },
                { word: 'but', prob: 10 }
            ],
            'rainy': [
                { word: 'and', prob: 32 },
                { word: ',', prob: 25 },
                { word: 'with', prob: 20 },
                { word: 'but', prob: 13 },
                { word: 'so', prob: 10 }
            ],
            'cold': [
                { word: 'and', prob: 30 },
                { word: ',', prob: 22 },
                { word: 'with', prob: 18 },
                { word: 'but', prob: 16 },
                { word: 'so', prob: 14 }
            ]
        };

        function selectWord(word) {
            // Update button states
            document.querySelectorAll('.word-button').forEach(btn => {
                btn.classList.remove('selected');
            });
            event.target.classList.add('selected');

            // Show predictions
            const probViz = document.getElementById('probabilityViz');
            const preds = predictions[word];

            probViz.innerHTML = '<p style="font-weight: 600; margin-bottom: 16px;">Predicted next words:</p>';

            preds.forEach(pred => {
                probViz.innerHTML += `
                    <div style="margin: 12px 0;">
                        <div style="display: flex; justify-content: space-between; margin-bottom: 4px;">
                            <span style="font-family: monospace; font-weight: 600;">${pred.word}</span>
                            <span style="color: var(--text-secondary);">${pred.prob}%</span>
                        </div>
                        <div class="probability-bar" style="width: ${pred.prob}%;">
                        </div>
                    </div>
                `;
            });
        }

        // ========== EMBEDDING VISUALIZATION (Slide 4) ==========
        function drawEmbeddingVisualization() {
            const canvas = document.getElementById('embeddingCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;

            // Clear canvas
            ctx.clearRect(0, 0, width, height);

            // Draw 3D-looking space
            ctx.strokeStyle = '#e2e8f0';
            ctx.lineWidth = 1;

            // Grid lines
            for (let i = 0; i < width; i += 50) {
                ctx.beginPath();
                ctx.moveTo(i, 0);
                ctx.lineTo(i, height);
                ctx.stroke();
            }
            for (let i = 0; i < height; i += 50) {
                ctx.beginPath();
                ctx.moveTo(0, i);
                ctx.lineTo(width, i);
                ctx.stroke();
            }

            // Draw word embeddings as points
            const words = [
                { text: 'cat', x: 150, y: 80, color: '#2563eb' },
                { text: 'dog', x: 180, y: 90, color: '#2563eb' },
                { text: 'kitten', x: 140, y: 95, color: '#2563eb' },
                { text: 'car', x: 350, y: 110, color: '#10b981' },
                { text: 'vehicle', x: 370, y: 100, color: '#10b981' },
                { text: 'happy', x: 250, y: 150, color: '#ef4444' },
                { text: 'joyful', x: 270, y: 160, color: '#ef4444' }
            ];

            words.forEach(word => {
                // Draw point
                ctx.fillStyle = word.color;
                ctx.beginPath();
                ctx.arc(word.x, word.y, 6, 0, Math.PI * 2);
                ctx.fill();

                // Draw label
                ctx.font = 'bold 14px Arial';
                ctx.fillText(word.text, word.x + 12, word.y + 5);
            });

            // Draw grouping circles
            ctx.strokeStyle = 'rgba(37, 99, 235, 0.3)';
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.arc(160, 90, 50, 0, Math.PI * 2);
            ctx.stroke();

            ctx.strokeStyle = 'rgba(16, 185, 129, 0.3)';
            ctx.beginPath();
            ctx.arc(360, 105, 45, 0, Math.PI * 2);
            ctx.stroke();

            ctx.strokeStyle = 'rgba(239, 68, 68, 0.3)';
            ctx.beginPath();
            ctx.arc(260, 155, 40, 0, Math.PI * 2);
            ctx.stroke();
        }

        // ========== ATTENTION VISUALIZATION (Slide 6) ==========
        function drawAttentionVisualization() {
            const canvas = document.getElementById('attentionCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;

            ctx.clearRect(0, 0, width, height);

            const words = ['The', 'cat', 'sat', 'on', 'the', 'mat'];
            const wordWidth = 80;
            const wordHeight = 40;
            const startX = 60;
            const startY = 80;
            const spacing = 90;

            // Attention matrix (simplified - each word attends to others)
            const attentionWeights = [
                [0.3, 0.1, 0.1, 0.1, 0.2, 0.2],  // The
                [0.2, 0.4, 0.2, 0.1, 0.05, 0.05], // cat
                [0.15, 0.25, 0.3, 0.15, 0.1, 0.05], // sat
                [0.1, 0.1, 0.2, 0.3, 0.15, 0.15],  // on
                [0.2, 0.1, 0.1, 0.15, 0.3, 0.15], // the
                [0.2, 0.15, 0.1, 0.15, 0.2, 0.2]  // mat
            ];

            let selectedWord = 1; // Default to 'cat'

            // Draw attention connections
            words.forEach((word, i) => {
                const x1 = startX + i * spacing + wordWidth / 2;
                const y1 = startY + wordHeight;

                attentionWeights[selectedWord].forEach((weight, j) => {
                    if (i === j) return; // Skip self-attention for clarity

                    const x2 = startX + j * spacing + wordWidth / 2;
                    const y2 = startY + 200;

                    const opacity = weight;
                    const lineWidth = 1 + weight * 4;

                    ctx.strokeStyle = `rgba(37, 99, 235, ${opacity})`;
                    ctx.lineWidth = lineWidth;

                    // Draw curved connection
                    ctx.beginPath();
                    ctx.moveTo(x1, y1);

                    const midY = (y1 + y2) / 2;
                    ctx.bezierCurveTo(x1, midY, x2, midY, x2, y2);

                    ctx.stroke();
                });
            });

            // Draw word boxes (source)
            words.forEach((word, i) => {
                const x = startX + i * spacing;
                const y = startY;

                if (i === selectedWord) {
                    ctx.fillStyle = '#2563eb';
                    ctx.fillRect(x - 2, y - 2, wordWidth + 4, wordHeight + 4);
                }

                ctx.fillStyle = i === selectedWord ? '#1e40af' : '#f8fafc';
                ctx.fillRect(x, y, wordWidth, wordHeight);

                ctx.strokeStyle = '#2563eb';
                ctx.lineWidth = 2;
                ctx.strokeRect(x, y, wordWidth, wordHeight);

                ctx.fillStyle = i === selectedWord ? '#ffffff' : '#0f172a';
                ctx.font = 'bold 16px Arial';
                ctx.textAlign = 'center';
                ctx.textBaseline = 'middle';
                ctx.fillText(word, x + wordWidth / 2, y + wordHeight / 2);
            });

            // Draw word boxes (target)
            words.forEach((word, i) => {
                const x = startX + i * spacing;
                const y = startY + 200;

                ctx.fillStyle = '#f8fafc';
                ctx.fillRect(x, y, wordWidth, wordHeight);

                ctx.strokeStyle = '#64748b';
                ctx.lineWidth = 1;
                ctx.strokeRect(x, y, wordWidth, wordHeight);

                ctx.fillStyle = '#0f172a';
                ctx.font = '14px Arial';
                ctx.textAlign = 'center';
                ctx.textBaseline = 'middle';
                ctx.fillText(word, x + wordWidth / 2, y + wordHeight / 2);
            });

            // Add labels
            ctx.fillStyle = '#64748b';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Query:', 10, startY + 20);
            ctx.fillText('Keys:', 10, startY + 220);

            // Add legend
            ctx.fillStyle = '#0f172a';
            ctx.font = '13px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('‚Üê Attention flows from highlighted word to all other words ‚Üí', width / 2, height - 30);
            ctx.fillStyle = '#64748b';
            ctx.font = '12px Arial';
            ctx.fillText('Line thickness represents attention strength', width / 2, height - 12);
        }

        // ========== TRAINING VISUALIZATION (Slide 7) ==========
        function drawTrainingVisualization() {
            const canvas = document.getElementById('trainingCanvas');
            if (!canvas) return;

            const ctx = canvas.getContext('2d');
            const width = canvas.width;
            const height = canvas.height;

            ctx.clearRect(0, 0, width, height);

            // Draw training progress curve
            ctx.strokeStyle = '#2563eb';
            ctx.lineWidth = 3;
            ctx.beginPath();

            // Loss curve (decreasing over time)
            const points = 50;
            for (let i = 0; i < points; i++) {
                const x = 50 + (i / points) * (width - 100);
                // Exponential decay with some noise
                const baseY = height - 50 - (1 - Math.exp(-i / 10)) * (height - 100);
                const noise = Math.sin(i * 0.5) * 5;
                const y = baseY + noise;

                if (i === 0) {
                    ctx.moveTo(x, y);
                } else {
                    ctx.lineTo(x, y);
                }
            }
            ctx.stroke();

            // Draw axes
            ctx.strokeStyle = '#64748b';
            ctx.lineWidth = 2;

            // Y-axis
            ctx.beginPath();
            ctx.moveTo(50, 30);
            ctx.lineTo(50, height - 30);
            ctx.stroke();

            // X-axis
            ctx.beginPath();
            ctx.moveTo(50, height - 50);
            ctx.lineTo(width - 30, height - 50);
            ctx.stroke();

            // Labels
            ctx.fillStyle = '#0f172a';
            ctx.font = 'bold 14px Arial';
            ctx.textAlign = 'center';

            ctx.save();
            ctx.translate(20, height / 2);
            ctx.rotate(-Math.PI / 2);
            ctx.fillText('Prediction Error', 0, 0);
            ctx.restore();

            ctx.fillText('Training Steps (Billions)', width / 2, height - 10);

            // Add annotation
            ctx.fillStyle = '#2563eb';
            ctx.font = 'bold 16px Arial';
            ctx.textAlign = 'left';
            ctx.fillText('Model gets better over time!', width - 250, 60);

            // Draw arrow pointing to curve
            ctx.strokeStyle = '#2563eb';
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.moveTo(width - 140, 70);
            ctx.lineTo(width - 100, 100);
            ctx.stroke();

            // Arrowhead
            ctx.fillStyle = '#2563eb';
            ctx.beginPath();
            ctx.moveTo(width - 100, 100);
            ctx.lineTo(width - 105, 92);
            ctx.lineTo(width - 95, 95);
            ctx.closePath();
            ctx.fill();
        }

        // Initialize carousel
        initCarousel();
    </script>
</body>
</html>
