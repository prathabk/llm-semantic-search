<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Models with Ollama - Semantic Search Course</title>
    <link rel="stylesheet" href="static/css/style.css">
    <link rel="stylesheet" href="static/css/carousel.css">
    <style>
        canvas {
            border: 1px solid var(--border);
            border-radius: 8px;
            background: var(--surface);
            display: block;
            margin: 20px auto;
            box-shadow: 0 1px 3px var(--shadow);
        }

        .title-slide {
            text-align: center;
            padding: 80px 20px;
        }

        .title-slide h1 {
            font-size: 3em;
            margin-bottom: 16px;
        }

        .title-slide .subtitle {
            font-size: 1.5em;
            color: var(--text-secondary);
            margin-bottom: 40px;
        }

        .title-slide .course-info {
            max-width: 700px;
            margin: 0 auto;
            font-size: 1.125em;
            line-height: 1.8;
        }

        .code-block {
            background: #1e293b;
            color: #e2e8f0;
            padding: 20px;
            border-radius: 8px;
            font-family: 'SF Mono', Monaco, 'Courier New', monospace;
            font-size: 0.9375em;
            overflow-x: auto;
            margin: 20px 0;
            line-height: 1.6;
        }

        .code-block .comment {
            color: #94a3b8;
        }

        .code-block .keyword {
            color: #f472b6;
        }

        .code-block .string {
            color: #6ee7b7;
        }

        .code-block .function {
            color: #7dd3fc;
        }

        .architecture-box {
            background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%);
            border: 2px solid var(--border);
            border-radius: 12px;
            padding: 24px;
            margin: 20px 0;
        }

        .comparison-table {
            width: 100%;
            margin: 20px 0;
            border-collapse: collapse;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 16px;
            text-align: left;
            border-bottom: 1px solid var(--border);
        }

        .comparison-table th {
            background: var(--surface);
            font-weight: 600;
            color: var(--text);
        }

        .comparison-table tr:hover {
            background: var(--bg);
        }

        .feature-badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.875em;
            font-weight: 500;
            margin: 4px;
        }

        .badge-pro {
            background: #dcfce7;
            color: #166534;
        }

        .badge-con {
            background: #fee2e2;
            color: #991b1b;
        }

        .model-card {
            background: var(--surface);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 20px;
            margin: 12px 0;
            transition: transform 0.2s;
        }

        .model-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px var(--shadow);
        }

        .model-card h4 {
            color: var(--primary);
            margin-bottom: 8px;
        }

        .terminal-output {
            background: #0f172a;
            color: #10b981;
            padding: 16px;
            border-radius: 8px;
            font-family: 'SF Mono', Monaco, monospace;
            font-size: 0.875em;
            margin: 16px 0;
            line-height: 1.6;
        }

        .step-indicator {
            display: inline-block;
            width: 32px;
            height: 32px;
            background: var(--primary);
            color: white;
            border-radius: 50%;
            text-align: center;
            line-height: 32px;
            font-weight: 600;
            margin-right: 12px;
        }

        .workflow-step {
            margin: 24px 0;
            padding-left: 44px;
            position: relative;
        }

        .workflow-step::before {
            content: '';
            position: absolute;
            left: 16px;
            top: 40px;
            bottom: -24px;
            width: 2px;
            background: var(--border);
        }

        .workflow-step:last-child::before {
            display: none;
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="index.html" class="home-link">‚Üê Back to Home</a>

        <div class="progress-bar">
            <div class="progress-fill" id="progressFill"></div>
        </div>

        <div class="carousel-container">
            <div class="carousel-wrapper" id="carouselWrapper">

                <!-- Slide 1: Title -->
                <div class="slide">
                    <div class="slide-content title-slide">
                        <h1>ü§ñ LLM Models with Ollama</h1>
                        <p class="subtitle">Running Large Language Models Locally</p>
                        <div class="course-info">
                            <p>Large Language Models (LLMs) are powerful AI systems that understand and generate human-like text. With Ollama, you can run these models locally on your machine for generating embeddings and semantic understanding.</p>
                            <p>This module covers how LLMs work, what Ollama is, and how to use it for semantic search applications.</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 2: What are LLMs? -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>What are Large Language Models?</h2>
                            <p class="slide-subtitle">Understanding the Foundation</p>
                        </div>

                        <p><strong>Large Language Models (LLMs)</strong> are neural networks trained on massive amounts of text data to understand and generate human language. They learn patterns, relationships, and meanings from billions of words.</p>

                        <div class="architecture-box">
                            <h3 style="margin-top: 0;">Key Capabilities:</h3>
                            <ul style="line-height: 2;">
                                <li><strong>Text Generation:</strong> Write coherent, contextual text</li>
                                <li><strong>Understanding:</strong> Comprehend meaning and intent</li>
                                <li><strong>Translation:</strong> Convert between languages</li>
                                <li><strong>Embeddings:</strong> Create semantic vector representations</li>
                                <li><strong>Reasoning:</strong> Answer questions and solve problems</li>
                            </ul>
                        </div>

                        <canvas id="llmOverviewCanvas" width="700" height="300"></canvas>

                        <div class="example-box">
                            <h3>Real-World Examples</h3>
                            <p><strong>GPT-4:</strong> OpenAI's powerful model for chat and code<br>
                            <strong>LLaMA:</strong> Meta's open-source family of models<br>
                            <strong>Mistral:</strong> Efficient, high-performance models<br>
                            <strong>Gemma:</strong> Google's lightweight models</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 3: How LLMs Create Embeddings -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>How LLMs Create Embeddings</h2>
                            <p class="slide-subtitle">From Text to Vectors</p>
                        </div>

                        <p>LLMs use a <strong>transformer architecture</strong> with layers of neural networks that process text through attention mechanisms. Each layer captures different levels of meaning.</p>

                        <canvas id="transformerCanvas" width="700" height="380"></canvas>

                        <div class="architecture-box">
                            <h3 style="margin-top: 0;">The Process:</h3>
                            <ol style="line-height: 2;">
                                <li><strong>Tokenization:</strong> Split text into tokens (words/subwords)</li>
                                <li><strong>Input Embeddings:</strong> Convert tokens to initial vectors</li>
                                <li><strong>Attention Layers:</strong> Analyze relationships between all tokens</li>
                                <li><strong>Layer Processing:</strong> Refine understanding through multiple layers</li>
                                <li><strong>Output Embedding:</strong> Extract final vector representation</li>
                            </ol>
                        </div>

                        <div class="example-box">
                            <h3>Why Transformer Architecture?</h3>
                            <p><strong>Attention Mechanism:</strong> Can focus on relevant words regardless of distance<br>
                            <strong>Parallel Processing:</strong> Faster training than sequential models<br>
                            <strong>Context Understanding:</strong> Captures long-range dependencies</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 4: What is Ollama? -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>What is Ollama?</h2>
                            <p class="slide-subtitle">Running LLMs Made Simple</p>
                        </div>

                        <p><strong>Ollama</strong> is an open-source tool that makes it easy to run large language models locally on your machine. Think of it as "Docker for LLMs" - simple, fast, and powerful.</p>

                        <canvas id="ollamaArchCanvas" width="700" height="320"></canvas>

                        <div class="architecture-box">
                            <h3 style="margin-top: 0;">Why Use Ollama?</h3>
                            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 16px; margin-top: 16px;">
                                <div>
                                    <h4 style="color: var(--primary); margin-bottom: 8px;">‚úÖ Benefits</h4>
                                    <ul style="line-height: 1.8;">
                                        <li>Privacy: Data stays local</li>
                                        <li>No API costs</li>
                                        <li>Offline capability</li>
                                        <li>Fast inference</li>
                                        <li>Easy model switching</li>
                                    </ul>
                                </div>
                                <div>
                                    <h4 style="color: var(--primary); margin-bottom: 8px;">‚öôÔ∏è Features</h4>
                                    <ul style="line-height: 1.8;">
                                        <li>One-command install</li>
                                        <li>Model library</li>
                                        <li>REST API</li>
                                        <li>GPU acceleration</li>
                                        <li>Model customization</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="example-box">
                            <h3>Installation</h3>
                            <div class="code-block">
<span class="comment"># macOS / Linux</span>
curl -fsSL https://ollama.com/install.sh | sh

<span class="comment"># Windows - Download installer from ollama.com</span>

<span class="comment"># Verify installation</span>
ollama --version
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 5: Local vs Cloud LLMs -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Local vs Cloud LLMs</h2>
                            <p class="slide-subtitle">Choosing the Right Approach</p>
                        </div>

                        <table class="comparison-table">
                            <thead>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Local (Ollama)</th>
                                    <th>Cloud (OpenAI, etc.)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>Privacy</strong></td>
                                    <td><span class="feature-badge badge-pro">Complete control</span></td>
                                    <td><span class="feature-badge badge-con">Data sent to API</span></td>
                                </tr>
                                <tr>
                                    <td><strong>Cost</strong></td>
                                    <td><span class="feature-badge badge-pro">Free (hardware only)</span></td>
                                    <td><span class="feature-badge badge-con">Pay per token</span></td>
                                </tr>
                                <tr>
                                    <td><strong>Speed</strong></td>
                                    <td><span class="feature-badge badge-pro">Fast (local GPU)</span></td>
                                    <td>Depends on network</td>
                                </tr>
                                <tr>
                                    <td><strong>Model Size</strong></td>
                                    <td><span class="feature-badge badge-con">Limited by hardware</span></td>
                                    <td><span class="feature-badge badge-pro">Largest models</span></td>
                                </tr>
                                <tr>
                                    <td><strong>Availability</strong></td>
                                    <td><span class="feature-badge badge-pro">Works offline</span></td>
                                    <td><span class="feature-badge badge-con">Requires internet</span></td>
                                </tr>
                                <tr>
                                    <td><strong>Setup</strong></td>
                                    <td>One-time install</td>
                                    <td>API key needed</td>
                                </tr>
                            </tbody>
                        </table>

                        <div class="example-box" style="margin-top: 24px;">
                            <h3>When to Use Local LLMs?</h3>
                            <p><strong>‚úì Perfect for:</strong> Sensitive data, high-volume processing, offline needs, development/testing, embedding generation</p>
                            <p><strong>‚úó Consider cloud if:</strong> Need largest models, limited hardware, infrequent use</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 6: Using Ollama - Basics -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Using Ollama - Basic Commands</h2>
                            <p class="slide-subtitle">Getting Started</p>
                        </div>

                        <div class="workflow-step">
                            <span class="step-indicator">1</span>
                            <h3 style="display: inline-block; margin: 0;">Pull a Model</h3>
                            <div class="code-block" style="margin-top: 12px;">
<span class="comment"># Download a model (first time)</span>
ollama pull llama2

<span class="comment"># Download an embedding model</span>
ollama pull nomic-embed-text
                            </div>
                        </div>

                        <div class="workflow-step">
                            <span class="step-indicator">2</span>
                            <h3 style="display: inline-block; margin: 0;">Run a Model</h3>
                            <div class="code-block" style="margin-top: 12px;">
<span class="comment"># Interactive chat</span>
ollama run llama2

<span class="comment"># Single prompt</span>
ollama run llama2 <span class="string">"Explain embeddings in simple terms"</span>
                            </div>
                        </div>

                        <div class="workflow-step">
                            <span class="step-indicator">3</span>
                            <h3 style="display: inline-block; margin: 0;">List Models</h3>
                            <div class="code-block" style="margin-top: 12px;">
<span class="comment"># See installed models</span>
ollama list
                            </div>
                            <div class="terminal-output">
NAME                    ID              SIZE    MODIFIED
llama2:latest           78e26419b446    3.8 GB  2 hours ago
nomic-embed-text:latest 0a109f422b47    274 MB  1 day ago
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Slide 7: Generating Embeddings with Ollama -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Generating Embeddings with Ollama</h2>
                            <p class="slide-subtitle">Practical Implementation</p>
                        </div>

                        <p>Ollama provides a simple API for generating embeddings. Let's see how to use it in Python:</p>

                        <div class="code-block">
<span class="keyword">import</span> requests
<span class="keyword">import</span> json

<span class="keyword">def</span> <span class="function">get_embedding</span>(text, model=<span class="string">"nomic-embed-text"</span>):
    <span class="comment">"""Generate embedding for text using Ollama"""</span>
    url = <span class="string">"http://localhost:11434/api/embeddings"</span>

    payload = {
        <span class="string">"model"</span>: model,
        <span class="string">"prompt"</span>: text
    }

    response = requests.post(url, json=payload)
    embedding = response.json()[<span class="string">"embedding"</span>]

    <span class="keyword">return</span> embedding

<span class="comment"># Example usage</span>
text = <span class="string">"Machine learning is fascinating"</span>
embedding = get_embedding(text)

<span class="keyword">print</span>(<span class="string">f"Embedding dimension: </span>{len(embedding)}<span class="string">"</span>)
<span class="keyword">print</span>(<span class="string">f"First 5 values: </span>{embedding[:5]}<span class="string">"</span>)
                        </div>

                        <div class="terminal-output">
Embedding dimension: 768
First 5 values: [0.023, -0.891, 0.445, -0.123, 0.667]
                        </div>

                        <div class="example-box">
                            <h3>Key Points</h3>
                            <ul>
                                <li><strong>Ollama Server:</strong> Must be running (starts on port 11434)</li>
                                <li><strong>Model Choice:</strong> Use embedding-specific models for best results</li>
                                <li><strong>Dimension:</strong> Each model produces fixed-size vectors (e.g., 768 or 1024)</li>
                                <li><strong>Consistency:</strong> Same text always produces same embedding</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- Slide 8: Best Embedding Models -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Choosing an Embedding Model</h2>
                            <p class="slide-subtitle">Popular Options in Ollama</p>
                        </div>

                        <div class="model-card">
                            <h4>nomic-embed-text (Recommended)</h4>
                            <p><strong>Size:</strong> 274 MB | <strong>Dimensions:</strong> 768</p>
                            <p>Excellent general-purpose embedding model. Fast, accurate, and efficient. Great for semantic search, RAG, and clustering.</p>
                            <div class="code-block">ollama pull nomic-embed-text</div>
                        </div>

                        <div class="model-card">
                            <h4>mxbai-embed-large</h4>
                            <p><strong>Size:</strong> 669 MB | <strong>Dimensions:</strong> 1024</p>
                            <p>Larger model with higher accuracy. Best for production applications where quality is critical. Slightly slower but more precise.</p>
                            <div class="code-block">ollama pull mxbai-embed-large</div>
                        </div>

                        <div class="model-card">
                            <h4>all-minilm</h4>
                            <p><strong>Size:</strong> 46 MB | <strong>Dimensions:</strong> 384</p>
                            <p>Lightweight and fast. Good for resource-constrained environments or when speed is critical. Lower accuracy than larger models.</p>
                            <div class="code-block">ollama pull all-minilm</div>
                        </div>

                        <div class="example-box">
                            <h3>Selection Guide</h3>
                            <p><strong>Speed Priority:</strong> all-minilm<br>
                            <strong>Balanced:</strong> nomic-embed-text<br>
                            <strong>Accuracy Priority:</strong> mxbai-embed-large</p>
                        </div>
                    </div>
                </div>

                <!-- Slide 9: Batch Processing -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Batch Embedding Generation</h2>
                            <p class="slide-subtitle">Processing Multiple Texts Efficiently</p>
                        </div>

                        <p>For real applications, you'll need to generate embeddings for many documents. Here's an efficient approach:</p>

                        <div class="code-block">
<span class="keyword">import</span> requests
<span class="keyword">from</span> typing <span class="keyword">import</span> List

<span class="keyword">def</span> <span class="function">batch_embed</span>(texts: List[str], model=<span class="string">"nomic-embed-text"</span>):
    <span class="comment">"""Generate embeddings for multiple texts"""</span>
    url = <span class="string">"http://localhost:11434/api/embeddings"</span>
    embeddings = []

    <span class="keyword">for</span> i, text <span class="keyword">in</span> enumerate(texts):
        payload = {<span class="string">"model"</span>: model, <span class="string">"prompt"</span>: text}
        response = requests.post(url, json=payload)
        embedding = response.json()[<span class="string">"embedding"</span>]
        embeddings.append(embedding)

        <span class="comment"># Progress indicator</span>
        <span class="keyword">if</span> (i + 1) % 100 == 0:
            <span class="keyword">print</span>(<span class="string">f"Processed </span>{i + 1}<span class="string">/</span>{len(texts)}<span class="string"> documents"</span>)

    <span class="keyword">return</span> embeddings

<span class="comment"># Example usage</span>
documents = [
    <span class="string">"Python is a versatile programming language"</span>,
    <span class="string">"Machine learning enables computers to learn"</span>,
    <span class="string">"Neural networks are inspired by the brain"</span>,
    <span class="string">"Data science combines statistics and programming"</span>
]

embeddings = batch_embed(documents)
<span class="keyword">print</span>(<span class="string">f"Generated </span>{len(embeddings)}<span class="string"> embeddings"</span>)
                        </div>

                        <div class="example-box">
                            <h3>Performance Tips</h3>
                            <ul>
                                <li><strong>GPU Acceleration:</strong> Ollama automatically uses GPU if available</li>
                                <li><strong>Batch Size:</strong> Process in chunks for large datasets</li>
                                <li><strong>Caching:</strong> Store embeddings in database to avoid regeneration</li>
                                <li><strong>Parallel Processing:</strong> Use async requests for faster throughput</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- Slide 10: Integration with Semantic Search -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Complete Workflow</h2>
                            <p class="slide-subtitle">LLMs in Semantic Search Pipeline</p>
                        </div>

                        <canvas id="workflowCanvas" width="700" height="320"></canvas>

                        <div class="architecture-box">
                            <h3 style="margin-top: 20px;">The Complete Pipeline:</h3>
                            <ol style="line-height: 2;">
                                <li><strong>Document Collection:</strong> Gather text documents to search</li>
                                <li><strong>Embedding Generation:</strong> Use Ollama to convert text to vectors</li>
                                <li><strong>Vector Storage:</strong> Store embeddings in vector database (Typesense)</li>
                                <li><strong>Query Processing:</strong> Convert search query to embedding</li>
                                <li><strong>Similarity Search:</strong> Find closest vectors in database</li>
                                <li><strong>Results:</strong> Return most relevant documents</li>
                            </ol>
                        </div>

                        <div class="code-block">
<span class="comment"># Simplified semantic search workflow</span>

<span class="comment"># 1. Generate embeddings for documents</span>
doc_embeddings = [get_embedding(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> documents]

<span class="comment"># 2. Store in vector database (covered in next module)</span>
vector_db.store(documents, doc_embeddings)

<span class="comment"># 3. Process search query</span>
query = <span class="string">"how do neural networks work?"</span>
query_embedding = get_embedding(query)

<span class="comment"># 4. Find similar documents</span>
results = vector_db.search(query_embedding, top_k=5)
                        </div>
                    </div>
                </div>

                <!-- Slide 11: Key Takeaways -->
                <div class="slide">
                    <div class="slide-content">
                        <div class="slide-header">
                            <h2>Key Takeaways</h2>
                            <p class="slide-subtitle">What You've Learned</p>
                        </div>

                        <ul style="font-size: 1.125em; line-height: 2.2; margin-top: 40px;">
                            <li><strong>LLMs</strong> are powerful models that understand and generate language through transformer architecture and attention mechanisms</li>

                            <li><strong>Ollama</strong> makes running LLMs locally simple, providing privacy, cost savings, and offline capability</li>

                            <li><strong>Local vs Cloud</strong> each has tradeoffs - local offers privacy and control, cloud offers largest models</li>

                            <li><strong>Embedding generation</strong> is straightforward with Ollama's API - just POST to the endpoint</li>

                            <li><strong>Model selection</strong> depends on your needs - balance speed, accuracy, and resource constraints</li>

                            <li><strong>Integration</strong> with vector databases completes the semantic search pipeline</li>
                        </ul>

                        <div style="text-align: center; margin-top: 60px;">
                            <a href="vector-database-slides.html" style="text-decoration: none;">
                                <button class="action-btn" style="padding: 16px 32px; font-size: 1.125em;">
                                    Next: Typesense Vector Database ‚Üí
                                </button>
                            </a>
                        </div>
                    </div>
                </div>

            </div>
        </div>

        <!-- Navigation Controls -->
        <div class="carousel-nav">
            <button id="prevBtn" onclick="prevSlide()">‚Üê Previous</button>

            <div class="nav-center">
                <div class="slide-indicators" id="slideIndicators"></div>
                <span class="slide-counter">
                    <span id="currentSlide">1</span> / <span id="totalSlides">11</span>
                </span>
            </div>

            <button id="nextBtn" onclick="nextSlide()">Next ‚Üí</button>
        </div>
    </div>

    <div class="keyboard-hint" id="keyboardHint">
        Use <kbd>‚Üê</kbd> <kbd>‚Üí</kbd> arrow keys to navigate
    </div>

    <script>
        // Carousel State
        let currentSlide = 0;
        const totalSlides = 11;
        const carouselWrapper = document.getElementById('carouselWrapper');
        const progressFill = document.getElementById('progressFill');
        const prevBtn = document.getElementById('prevBtn');
        const nextBtn = document.getElementById('nextBtn');

        function initCarousel() {
            updateSlide();
            createIndicators();
            setTimeout(() => {
                document.getElementById('keyboardHint').style.animation = 'fadeInOut 3s ease-in-out';
            }, 1000);
        }

        function nextSlide() {
            if (currentSlide < totalSlides - 1) {
                currentSlide++;
                updateSlide();
            }
        }

        function prevSlide() {
            if (currentSlide > 0) {
                currentSlide--;
                updateSlide();
            }
        }

        function goToSlide(index) {
            currentSlide = index;
            updateSlide();
        }

        function updateSlide() {
            const offset = -currentSlide * 100;
            carouselWrapper.style.transform = `translateX(${offset}%)`;

            const progress = ((currentSlide + 1) / totalSlides) * 100;
            progressFill.style.width = `${progress}%`;

            document.getElementById('currentSlide').textContent = currentSlide + 1;

            prevBtn.disabled = currentSlide === 0;
            nextBtn.disabled = currentSlide === totalSlides - 1;

            updateIndicators();

            // Reinitialize canvas visualizations
            setTimeout(() => {
                if (currentSlide === 1) drawLLMOverview();
                if (currentSlide === 2) drawTransformer();
                if (currentSlide === 3) drawOllamaArch();
                if (currentSlide === 9) drawWorkflow();
            }, 100);
        }

        function createIndicators() {
            const container = document.getElementById('slideIndicators');
            for (let i = 0; i < totalSlides; i++) {
                const indicator = document.createElement('div');
                indicator.className = 'slide-indicator';
                indicator.onclick = () => goToSlide(i);
                container.appendChild(indicator);
            }
            updateIndicators();
        }

        function updateIndicators() {
            const indicators = document.querySelectorAll('.slide-indicator');
            indicators.forEach((indicator, index) => {
                indicator.classList.toggle('active', index === currentSlide);
            });
        }

        document.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowRight') nextSlide();
            if (e.key === 'ArrowLeft') prevSlide();
        });

        initCarousel();

        // Canvas Visualizations

        function drawLLMOverview() {
            const canvas = document.getElementById('llmOverviewCanvas');
            if (!canvas) return;
            const ctx = canvas.getContext('2d');

            // Clear canvas
            ctx.fillStyle = '#ffffff';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            // Draw text input
            ctx.fillStyle = '#ef4444';
            ctx.font = 'bold 16px Arial';
            ctx.fillText('Input Text', 40, 150);
            roundRect(ctx, 30, 100, 120, 80, 8, '#fee2e2', '#ef4444');
            ctx.fillStyle = '#0f172a';
            ctx.font = '14px Arial';
            ctx.fillText('"cat is cute"', 45, 140);

            // Arrow
            drawArrow(ctx, 160, 140, 220, 140, '#64748b', '', 2);

            // LLM box
            ctx.fillStyle = '#2563eb';
            ctx.font = 'bold 16px Arial';
            ctx.fillText('LLM Model', 260, 80);
            roundRect(ctx, 230, 100, 180, 80, 8, '#dbeafe', '#2563eb');
            ctx.fillStyle = '#0f172a';
            ctx.font = '14px Arial';
            ctx.fillText('Transformer', 265, 130);
            ctx.fillText('Neural Network', 265, 150);
            ctx.font = '12px Arial';
            ctx.fillText('Billions of parameters', 255, 170);

            // Arrow
            drawArrow(ctx, 420, 140, 480, 140, '#64748b', '', 2);

            // Output embedding
            ctx.fillStyle = '#10b981';
            ctx.font = 'bold 16px Arial';
            ctx.fillText('Embedding', 510, 90);
            roundRect(ctx, 490, 100, 180, 80, 8, '#d1fae5', '#10b981');
            ctx.fillStyle = '#0f172a';
            ctx.font = '12px monospace';
            ctx.fillText('[0.12, -0.45, ...', 510, 130);
            ctx.fillText(' 0.89, 0.23, ...', 510, 145);
            ctx.fillText(' ..., 0.67]', 510, 160);
            ctx.font = '11px Arial';
            ctx.fillText('(768 dimensions)', 520, 175);

            // Bottom explanation
            ctx.fillStyle = '#64748b';
            ctx.font = '14px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('LLMs transform text into high-dimensional vectors that capture semantic meaning', 350, 230);
            ctx.fillText('Same meaning ‚Üí Similar vectors  |  Different meaning ‚Üí Distant vectors', 350, 255);
            ctx.textAlign = 'left';
        }

        function drawTransformer() {
            const canvas = document.getElementById('transformerCanvas');
            if (!canvas) return;
            const ctx = canvas.getContext('2d');

            ctx.fillStyle = '#ffffff';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const layerHeight = 50;
            const layerWidth = 400;
            const startX = 150;
            const startY = 50;

            // Input layer
            ctx.fillStyle = '#ef4444';
            ctx.font = 'bold 14px Arial';
            ctx.fillText('Tokens: ["cat", "is", "cute"]', startX + 80, startY - 10);
            roundRect(ctx, startX, startY, layerWidth, layerHeight, 8, '#fee2e2', '#ef4444');
            ctx.fillStyle = '#0f172a';
            ctx.font = '13px Arial';
            ctx.fillText('Input Tokens ‚Üí Initial Vectors', startX + 100, startY + 30);

            // Arrow + label
            drawArrow(ctx, startX + 200, startY + layerHeight, startX + 200, startY + layerHeight + 25, '#64748b', '', 2);

            // Attention Layer 1
            const layer1Y = startY + layerHeight + 30;
            roundRect(ctx, startX, layer1Y, layerWidth, layerHeight, 8, '#dbeafe', '#2563eb');
            ctx.fillStyle = '#0f172a';
            ctx.font = '13px Arial';
            ctx.fillText('Attention Layer 1: Analyze word relationships', startX + 70, layer1Y + 30);

            // Arrow
            drawArrow(ctx, startX + 200, layer1Y + layerHeight, startX + 200, layer1Y + layerHeight + 25, '#64748b', '', 2);

            // Attention Layer 2
            const layer2Y = layer1Y + layerHeight + 30;
            roundRect(ctx, startX, layer2Y, layerWidth, layerHeight, 8, '#dbeafe', '#2563eb');
            ctx.fillStyle = '#0f172a';
            ctx.fillText('Attention Layer 2: Refine understanding', startX + 80, layer2Y + 30);

            // Arrow
            drawArrow(ctx, startX + 200, layer2Y + layerHeight, startX + 200, layer2Y + layerHeight + 25, '#64748b', '', 2);

            // Output layer
            const outputY = layer2Y + layerHeight + 30;
            roundRect(ctx, startX, outputY, layerWidth, layerHeight, 8, '#d1fae5', '#10b981');
            ctx.fillStyle = '#0f172a';
            ctx.fillText('Output: 768-dimensional embedding vector', startX + 75, outputY + 30);

            // Side annotation
            ctx.fillStyle = '#64748b';
            ctx.font = '12px Arial';
            ctx.fillText('12-48 layers', startX - 80, layer1Y + 50);
            ctx.fillText('in full models', startX - 80, layer1Y + 65);

            // Bottom note
            ctx.fillStyle = '#2563eb';
            ctx.font = 'bold 13px Arial';
            ctx.fillText('Each layer captures different levels of meaning and context', startX + 40, outputY + 75);
        }

        function drawOllamaArch() {
            const canvas = document.getElementById('ollamaArchCanvas');
            if (!canvas) return;
            const ctx = canvas.getContext('2d');

            ctx.fillStyle = '#ffffff';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            // Your Application
            ctx.fillStyle = '#2563eb';
            ctx.font = 'bold 16px Arial';
            ctx.fillText('Your Application', 70, 90);
            roundRect(ctx, 50, 100, 180, 80, 8, '#dbeafe', '#2563eb');
            ctx.fillStyle = '#0f172a';
            ctx.font = '13px Arial';
            ctx.fillText('Python/JavaScript', 80, 130);
            ctx.fillText('Send API requests', 75, 150);

            // Arrow right
            drawArrow(ctx, 240, 140, 300, 140, '#64748b', '', 2);
            ctx.fillStyle = '#64748b';
            ctx.font = '12px Arial';
            ctx.fillText('HTTP', 260, 130);

            // Ollama Server
            ctx.fillStyle = '#10b981';
            ctx.font = 'bold 16px Arial';
            ctx.fillText('Ollama Server', 340, 90);
            roundRect(ctx, 310, 100, 180, 80, 8, '#d1fae5', '#10b981');
            ctx.fillStyle = '#0f172a';
            ctx.font = '13px Arial';
            ctx.fillText('Port: 11434', 350, 125);
            ctx.fillText('Manages models', 345, 145);
            ctx.fillText('Handles inference', 340, 165);

            // Arrow down
            drawArrow(ctx, 400, 190, 400, 230, '#64748b', '', 2);

            // Models
            const modelY = 240;
            const modelGap = 140;

            // Model 1
            roundRect(ctx, 60, modelY, 120, 60, 6, '#fef3c7', '#f59e0b');
            ctx.fillStyle = '#0f172a';
            ctx.font = 'bold 13px Arial';
            ctx.fillText('llama2', 90, modelY + 25);
            ctx.font = '11px Arial';
            ctx.fillText('3.8 GB', 95, modelY + 45);

            // Model 2
            roundRect(ctx, 60 + modelGap, modelY, 120, 60, 6, '#fef3c7', '#f59e0b');
            ctx.fillStyle = '#0f172a';
            ctx.font = 'bold 13px Arial';
            ctx.fillText('nomic-embed', 210, modelY + 25);
            ctx.font = '11px Arial';
            ctx.fillText('274 MB', 230, modelY + 45);

            // Model 3
            roundRect(ctx, 60 + modelGap * 2, modelY, 120, 60, 6, '#fef3c7', '#f59e0b');
            ctx.fillStyle = '#0f172a';
            ctx.font = 'bold 13px Arial';
            ctx.fillText('mistral', 365, modelY + 25);
            ctx.font = '11px Arial';
            ctx.fillText('4.1 GB', 385, modelY + 45);

            // Bottom label
            ctx.fillStyle = '#64748b';
            ctx.font = '13px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Downloaded Models (Stored Locally)', 350, modelY + 85);
            ctx.textAlign = 'left';
        }

        function drawWorkflow() {
            const canvas = document.getElementById('workflowCanvas');
            if (!canvas) return;
            const ctx = canvas.getContext('2d');

            ctx.fillStyle = '#ffffff';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            const boxWidth = 110;
            const boxHeight = 70;
            const startX = 40;
            const y = 125;
            const gap = 125;

            // Step 1: Documents
            roundRect(ctx, startX, y, boxWidth, boxHeight, 8, '#fee2e2', '#ef4444');
            ctx.fillStyle = '#0f172a';
            ctx.font = 'bold 12px Arial';
            ctx.textAlign = 'center';
            ctx.fillText('Documents', startX + boxWidth/2, y + 30);
            ctx.font = '11px Arial';
            ctx.fillText('Text data', startX + boxWidth/2, y + 50);

            // Arrow
            drawArrow(ctx, startX + boxWidth, y + 35, startX + gap, y + 35, '#64748b', '', 2);
            ctx.fillStyle = '#64748b';
            ctx.font = '10px Arial';
            ctx.fillText('encode', startX + boxWidth + 30, y + 28);

            // Step 2: Ollama
            const step2X = startX + gap;
            roundRect(ctx, step2X, y, boxWidth, boxHeight, 8, '#dbeafe', '#2563eb');
            ctx.fillStyle = '#0f172a';
            ctx.font = 'bold 12px Arial';
            ctx.fillText('Ollama LLM', step2X + boxWidth/2, y + 30);
            ctx.font = '11px Arial';
            ctx.fillText('Embeddings', step2X + boxWidth/2, y + 50);

            // Arrow
            drawArrow(ctx, step2X + boxWidth, y + 35, step2X + gap, y + 35, '#64748b', '', 2);
            ctx.fillStyle = '#64748b';
            ctx.font = '10px Arial';
            ctx.fillText('store', step2X + boxWidth + 35, y + 28);

            // Step 3: Vector DB
            const step3X = step2X + gap;
            roundRect(ctx, step3X, y, boxWidth, boxHeight, 8, '#d1fae5', '#10b981');
            ctx.fillStyle = '#0f172a';
            ctx.font = 'bold 12px Arial';
            ctx.fillText('Vector DB', step3X + boxWidth/2, y + 30);
            ctx.font = '11px Arial';
            ctx.fillText('Typesense', step3X + boxWidth/2, y + 50);

            // Arrow
            drawArrow(ctx, step3X + boxWidth, y + 35, step3X + gap, y + 35, '#64748b', '', 2);
            ctx.fillStyle = '#64748b';
            ctx.font = '10px Arial';
            ctx.fillText('search', step3X + boxWidth + 30, y + 28);

            // Step 4: Results
            const step4X = step3X + gap;
            roundRect(ctx, step4X, y, boxWidth, boxHeight, 8, '#fef3c7', '#f59e0b');
            ctx.fillStyle = '#0f172a';
            ctx.font = 'bold 12px Arial';
            ctx.fillText('Results', step4X + boxWidth/2, y + 30);
            ctx.font = '11px Arial';
            ctx.fillText('Top matches', step4X + boxWidth/2, y + 50);

            // Query flow (top)
            const queryY = 40;
            ctx.fillStyle = '#7c3aed';
            ctx.font = 'bold 14px Arial';
            ctx.fillText('Search Query', 280, queryY);

            // Query arrow to Ollama
            drawArrow(ctx, 350, queryY + 10, step2X + boxWidth/2, y - 10, '#7c3aed', '', 2);
            ctx.fillStyle = '#7c3aed';
            ctx.font = '10px Arial';
            ctx.fillText('encode query', 300, queryY + 35);

            ctx.textAlign = 'left';
        }

        // Helper function to draw rounded rectangles
        function roundRect(ctx, x, y, width, height, radius, fillColor, strokeColor) {
            ctx.beginPath();
            ctx.moveTo(x + radius, y);
            ctx.lineTo(x + width - radius, y);
            ctx.quadraticCurveTo(x + width, y, x + width, y + radius);
            ctx.lineTo(x + width, y + height - radius);
            ctx.quadraticCurveTo(x + width, y + height, x + width - radius, y + height);
            ctx.lineTo(x + radius, y + height);
            ctx.quadraticCurveTo(x, y + height, x, y + height - radius);
            ctx.lineTo(x, y + radius);
            ctx.quadraticCurveTo(x, y, x + radius, y);
            ctx.closePath();

            if (fillColor) {
                ctx.fillStyle = fillColor;
                ctx.fill();
            }
            if (strokeColor) {
                ctx.strokeStyle = strokeColor;
                ctx.lineWidth = 2;
                ctx.stroke();
            }
        }

        // Helper function to draw arrows
        function drawArrow(ctx, fromX, fromY, toX, toY, color, label = '', width = 3) {
            ctx.strokeStyle = color;
            ctx.fillStyle = color;
            ctx.lineWidth = width;

            // Arrow line
            ctx.beginPath();
            ctx.moveTo(fromX, fromY);
            ctx.lineTo(toX, toY);
            ctx.stroke();

            // Arrow head
            const angle = Math.atan2(toY - fromY, toX - fromX);
            ctx.beginPath();
            ctx.moveTo(toX, toY);
            ctx.lineTo(toX - 10 * Math.cos(angle - Math.PI / 6), toY - 10 * Math.sin(angle - Math.PI / 6));
            ctx.lineTo(toX - 10 * Math.cos(angle + Math.PI / 6), toY - 10 * Math.sin(angle + Math.PI / 6));
            ctx.closePath();
            ctx.fill();

            // Label
            if (label) {
                ctx.font = 'bold 12px Arial';
                ctx.fillText(label, toX + 10, toY - 10);
            }
        }
    </script>
</body>
</html>
